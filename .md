# Complete ML Plan for npm Package Risk Scoring System

## Table of Contents
1. [Data Collection Requirements](#1-data-collection-requirements)
2. [Data Structure & Format](#2-data-structure--format)
3. [Recommended ML Algorithms](#3-recommended-ml-algorithms)
4. [Risk Score Output Definition](#4-risk-score-output-definition)
5. [Deliverables](#5-deliverables)

---

## 1. Data Collection Requirements
### 1.1 Package Metadata

**What to collect:**

Package name, version, publish timestamp
Maintainer information (usernames, email domains, account age)
Repository URL (GitHub, GitLab, etc.)
License type
Download statistics (weekly, monthly downloads)
Version history (frequency of updates, version jumps)
Package size and file count
Installation scripts presence (preinstall, postinstall, preuninstall)

**Why needed:**
- Risk indicators: Newly created packages or maintainer accounts often correlate with malicious intent
- Reputation signals: High download counts and long-standing maintainers typically indicate lower risk
- Anomaly detection: Sudden version jumps or unusual update patterns can signal compromise

**Contribution to risk prediction:**
- Temporal features help identify suspicious timing patterns (e.g., typosquatting campaigns)
- Maintainer reputation provides trust signals
- Installation scripts are common attack vectors

**Example format:**
```json
{
  "name": "express",
  "version": "4.18.2",
  "published_at": "2023-10-08T15:30:00Z",
  "maintainers": [
    {
      "username": "dougwilson",
      "account_age_days": 3650,
      "packages_maintained": 45,
      "email_domain": "gmail.com"
    }
  ],
  "repository": "https://github.com/expressjs/express",
  "license": "MIT",
  "downloads_last_week": 25000000,
  "downloads_last_month": 100000000,
  "version_count": 278,
  "days_since_last_update": 45,
  "package_size_bytes": 209152,
  "has_install_scripts": false
}
```

### 1.2 Dependency Graph Features

**What to collect:**

Direct dependencies (with version constraints)
Transitive dependencies (full dependency tree)
Dependency depth (how many layers deep)
Number of total dependencies
Dependency overlap with known packages
Circular dependencies detection
Outdated dependencies count
Deprecated dependencies

**Why needed:**
- Attack surface: More dependencies = larger attack surface
- Supply chain risk: Malicious packages often hide in deep dependency trees
- Maintenance signals: Outdated dependencies suggest poor maintenance

**Contribution to risk prediction:**
- Graph centrality metrics identify critical packages in ecosystem
- Anomalous dependency patterns (e.g., one package depending on 500+ others) signal risk
- Known vulnerable dependencies propagate risk scores

**Example format:**
```json
{
  "package": "my-app",
  "version": "1.0.0",
  "direct_dependencies": [
    {"name": "express", "version": "^4.18.0", "risk_score": 12},
    {"name": "lodash", "version": "^4.17.21", "risk_score": 8}
  ],
  "dependency_metrics": {
    "total_dependencies": 247,
    "max_depth": 8,
    "outdated_count": 12,
    "deprecated_count": 2,
    "circular_dependencies": false,
    "avg_dependency_age_days": 456
  },
  "graph_features": {
    "betweenness_centrality": 0.032,
    "clustering_coefficient": 0.18
  }
}
```

### 1.3 Vulnerability Databases

**What to collect:**

CVE identifiers and CVSS scores
Vulnerability descriptions and severity
Exploitation status (proof-of-concept available, actively exploited)
Fixed versions
Time to patch (days between disclosure and fix)
Historical vulnerability count

**Sources:**
- National Vulnerability Database (NVD)
- OSV (Open Source Vulnerabilities)
- GitHub Security Advisories
- npm audit database
- Snyk vulnerability DB

**Why needed:**
- Direct risk measure: Known vulnerabilities are concrete risk factors
- Maintenance quality: How quickly vulnerabilities are patched indicates maintainer responsiveness

**Contribution to risk prediction:**
- CVSS scores provide quantified severity
- Unpatched critical vulnerabilities dramatically increase risk score
- Historical vulnerability density predicts future risk

**Example format:**
```json
{
  "package": "minimist",
  "version": "1.2.5",
  "vulnerabilities": [
    {
      "id": "CVE-2021-44906",
      "cvss_score": 5.6,
      "severity": "MODERATE",
      "published": "2022-03-11T00:00:00Z",
      "fixed_in": "1.2.6",
      "exploited_in_wild": false,
      "description": "Prototype pollution vulnerability"
    }
  ],
  "vulnerability_metrics": {
    "total_vulnerabilities": 3,
    "critical_count": 0,
    "high_count": 1,
    "medium_count": 2,
    "avg_time_to_patch_days": 15
  }
}
```

### 1.4 Static Code Analysis Features

**What to collect:**

Suspicious API usage (eval, Function constructor, child_process)
Network requests (http/https calls, websockets)
File system operations (read/write/delete)
Environment variable access
Code obfuscation indicators (high entropy strings, minified code without source maps)
Cryptographic operations
Base64 encoded strings
Code complexity metrics (cyclomatic complexity, lines of code)
Dead code percentage
Typosquatting similarity to popular packages

**Why needed:**
- Malicious behavior detection: Direct indicators of malicious intent (e.g., eval with obfuscated code)
- Code quality: Poor quality code may indicate rushed or malicious development

**Contribution to risk prediction:**
- Behavioral patterns (network + filesystem + obfuscation) strongly correlate with malware
- Typosquatting detection prevents dependency confusion attacks
- Code complexity helps distinguish legitimate from malicious packages

**Example format:**
```json
{
  "package": "suspicious-pkg",
  "version": "1.0.0",
  "static_analysis": {
    "dangerous_apis": {
      "eval_usage": 3,
      "child_process_exec": 2,
      "fs_write": 5,
      "network_requests": 8
    },
    "obfuscation_indicators": {
      "high_entropy_strings": 12,
      "base64_encoded": 5,
      "obfuscation_score": 0.78
    },
    "code_metrics": {
      "total_loc": 2400,
      "cyclomatic_complexity": 18,
      "dead_code_percentage": 0.05
    },
    "typosquatting": {
      "similar_to": "express",
      "edit_distance": 1,
      "is_typosquat_candidate": true
    }
  }
}
```

### 1.5 Behavioral Signals

**What to collect:**

Install script content and behavior
Post-install network activity
File modifications during installation
Process spawning during installation
Permission requests (sudo, root)
Runtime behavior from sandboxed execution
Environment fingerprinting attempts

**Why needed:**
- Dynamic analysis: Static analysis can miss runtime malicious behavior
- Installation attacks: Many npm attacks occur during package installation

**Contribution to risk prediction:**
- Installation scripts executing network calls are red flags
- Packages requesting elevated permissions are high risk
- Runtime behavior complements static analysis

**Example format:**
```json
{
  "package": "malicious-example",
  "version": "1.0.0",
  "behavioral_analysis": {
    "install_script": {
      "present": true,
      "size_bytes": 1024,
      "network_calls": [
        {"host": "malicious-domain.com", "protocol": "https"}
      ],
      "spawned_processes": ["curl", "bash"],
      "file_modifications": ["/etc/hosts"],
      "requires_sudo": true
    },
    "sandbox_execution": {
      "network_activity": true,
      "filesystem_writes": 8,
      "attempted_privilege_escalation": true,
      "environment_fingerprinting": true
    }
  }
}
```

### 1.6 Package Popularity & Maintainer Activity

**What to collect:**

GitHub stars, forks, watchers
Issue response time
Pull request merge rate
Commit frequency
Number of contributors
Community health score
Documentation quality
Test coverage
CI/CD presence
Maintainer response patterns

**Why needed:**
- Trust signals: Popular, well-maintained packages are typically safer
- Community oversight: Active communities catch malicious updates faster

**Contribution to risk prediction:**
- Low popularity + poor maintenance = higher risk
- Sudden changes in maintainer behavior can signal account compromise
- Active community engagement correlates with security vigilance

**Example format:**
```json
{
  "package": "react",
  "version": "18.2.0",
  "popularity_metrics": {
    "github_stars": 210000,
    "github_forks": 44000,
    "github_watchers": 6800,
    "npm_weekly_downloads": 18000000,
    "dependents_count": 15000000
  },
  "maintainer_activity": {
    "active_contributors": 1500,
    "commits_last_90_days": 145,
    "avg_issue_response_hours": 24,
    "pr_merge_rate": 0.65,
    "last_commit_days_ago": 3,
    "bus_factor": 8
  },
  "quality_indicators": {
    "has_tests": true,
    "test_coverage": 0.89,
    "has_ci": true,
    "documentation_score": 0.95
  }
}
```

### 1.7 Labeled Training Dataset

**What to collect:**

Confirmed malicious packages (from security reports, takedowns)
Benign packages (verified safe packages)
Label sources and confidence scores
Attack type classification (typosquatting, malware, backdoor, etc.)
Discovery date and reporting source

**Sources:**
- npm security advisories
- Open source malware repositories (MalOSS dataset)
- Security researcher reports
- Historical takedowns
- Community reports

**Why needed:**
- Supervised learning: Labels are essential for training classification models
- Evaluation: Ground truth for measuring model performance

**Example format:**
```json
{
  "package": "event-streamm",
  "version": "3.3.6",
  "label": "malicious",
  "confidence": 1.0,
  "attack_type": "backdoor",
  "discovery_date": "2018-11-26",
  "source": "npm_security_advisory",
  "description": "Bitcoin wallet stealing backdoor",
  "advisory_id": "NSP-1492"
}
```

---

## 2. Data Structure & Format

### 2.1 Raw Data Storage

Store raw collected data in document-oriented format (MongoDB/JSON) for flexibility:

```json
{
  "_id": "express@4.18.2",
  "collected_at": "2025-11-20T10:00:00Z",
  "metadata": { /* from section 1.1 */ },
  "dependencies": { /* from section 1.2 */ },
  "vulnerabilities": { /* from section 1.3 */ },
  "static_analysis": { /* from section 1.4 */ },
  "behavioral": { /* from section 1.5 */ },
  "popularity": { /* from section 1.6 */ },
  "label": { /* from section 1.7 */ }
}
```

### 2.2 Feature Engineering Pipeline

Transform raw data into ML-ready features:

**Numerical Features:**
```python
numerical_features = [
    # Metadata
    'account_age_days',
    'packages_maintained',
    'downloads_last_week',
    'downloads_last_month',
    'version_count',
    'days_since_last_update',
    'package_size_bytes',
    
    # Dependencies
    'total_dependencies',
    'max_depth',
    'outdated_count',
    'deprecated_count',
    'avg_dependency_age_days',
    'betweenness_centrality',
    'clustering_coefficient',
    
    # Vulnerabilities
    'total_vulnerabilities',
    'critical_count',
    'high_count',
    'medium_count',
    'avg_time_to_patch_days',
    'max_cvss_score',
    
    # Static Analysis
    'eval_usage',
    'child_process_exec',
    'fs_write',
    'network_requests',
    'high_entropy_strings',
    'base64_encoded',
    'obfuscation_score',
    'total_loc',
    'cyclomatic_complexity',
    'dead_code_percentage',
    'edit_distance_to_popular',
    
    # Behavioral
    'install_script_size_bytes',
    'network_calls_count',
    'spawned_processes_count',
    'file_modifications_count',
    
    # Popularity
    'github_stars',
    'github_forks',
    'dependents_count',
    'active_contributors',
    'commits_last_90_days',
    'avg_issue_response_hours',
    'pr_merge_rate',
    'test_coverage'
]
```

**Categorical Features:**
```python
categorical_features = [
    'license',                    # MIT, Apache-2.0, ISC, etc.
    'email_domain',               # gmail.com, company.com, etc.
    'repository_host',            # github.com, gitlab.com, none
    'has_install_scripts',        # true, false
    'circular_dependencies',      # true, false
    'is_typosquat_candidate',     # true, false
    'requires_sudo',              # true, false
    'network_activity',           # true, false
    'attempted_privilege_escalation',  # true, false
    'has_tests',                  # true, false
    'has_ci'                      # true, false
]
```

**Text Features (for NLP):**
```python
text_features = [
    'package_description',
    'readme_content',
    'code_comments',
    'commit_messages'
]
```

### 2.3 Training Data Format (Tabular)

Final feature matrix for training:

```csv
package_name,version,account_age_days,downloads_last_week,total_dependencies,max_cvss_score,obfuscation_score,github_stars,has_install_scripts,license,label,risk_score
express,4.18.2,3650,25000000,247,0,0.05,60000,0,MIT,benign,12
event-streamm,3.3.6,120,50000,15,0,0.85,200,1,MIT,malicious,95
lodash,4.17.21,3800,30000000,0,5.6,0.02,55000,0,MIT,benign,8
```

### 2.4 Graph Representation (for GNN)

Dependency graph in edge-list format:

```json
{
  "nodes": [
    {"id": "express@4.18.2", "features": [/* numerical features */]},
    {"id": "body-parser@1.20.1", "features": [/* numerical features */]},
    {"id": "cookie@0.5.0", "features": [/* numerical features */]}
  ],
  "edges": [
    {"source": "express@4.18.2", "target": "body-parser@1.20.1", "type": "depends_on"},
    {"source": "express@4.18.2", "target": "cookie@0.5.0", "type": "depends_on"}
  ]
}
```

### 2.5 Feature Vector Structure

Complete feature vector for one package:

```python
{
    'numerical': np.array([3650, 25000000, 247, ...]),  # 50+ features
    'categorical': np.array([0, 2, 1, ...]),            # encoded categories
    'text_embedding': np.array([0.23, -0.45, ...]),     # 384-dim from BERT
    'graph_embedding': np.array([0.12, 0.89, ...]),     # 128-dim from GNN
    'label': 0,                                          # 0=benign, 1=malicious
    'risk_score': 12                                     # ground truth if available
}
```

---

## 3. Recommended ML Algorithms

### 3.1 Ensemble of Traditional ML Models (Primary Approach)

#### XGBoost (Gradient Boosted Trees)

**Why this fits:**

- Excellent for tabular data with mixed feature types
- Handles non-linear relationships and feature interactions
- Built-in feature importance for explainability
- Robust to outliers and missing values
- High performance with structured features

**Features used:**
- All numerical features (50+ dimensions)
- Encoded categorical features
- Aggregated graph metrics (centrality, clustering)

**Configuration:**
```python
xgb_model = XGBClassifier(
    objective='binary:logistic',  # or 'reg:squarederror' for regression
    max_depth=10,
    learning_rate=0.05,
    n_estimators=500,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=10  # handle class imbalance
)
```

**Output:** Risk probability (0-1) and feature importance scores

#### Random Forest

**Why this fits:**

- Provides complementary predictions to XGBoost
- Excellent for feature selection
- Natural ensemble approach reduces overfitting
- Handles high-dimensional data well

**Features used:**
- Same as XGBoost
- Can handle raw categorical features

**Configuration:**
```python
rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=15,
    min_samples_split=10,
    min_samples_leaf=5,
    class_weight='balanced'
)
```

**Output:** Risk probability and out-of-bag error estimates

#### Logistic Regression (with Regularization)

**Why this fits:**

- Baseline model for interpretability
- Fast training and inference
- Coefficients directly interpretable as risk factors
- Good for linear relationships

**Features used:**
- Normalized numerical features
- One-hot encoded categorical features

**Configuration:**
```python
lr_model = LogisticRegression(
    penalty='elasticnet',
    l1_ratio=0.5,
    C=1.0,
    solver='saga',
    class_weight='balanced'
)
```

**Output:** Risk probability and feature coefficients

### 3.2 Graph Neural Networks (GNN) for Dependency Analysis

#### Graph Attention Networks (GAT)

**Why this fits:**

- Dependency graphs are natural graph structures
- Attention mechanism learns which dependencies matter most
- Propagates risk through dependency chains
- Captures transitive risk relationships

**Features used:**
- Node features: package-level features (metadata, vulnerabilities, static analysis)
- Edge features: dependency version constraints, relationship type
- Graph structure: directed edges representing dependencies

**Architecture:**
```python
class PackageGNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GATConv(in_channels=50, out_channels=128, heads=8)
        self.conv2 = GATConv(in_channels=128*8, out_channels=64, heads=4)
        self.conv3 = GATConv(in_channels=64*4, out_channels=32, heads=1)
        self.fc = torch.nn.Linear(32, 1)
    
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        return torch.sigmoid(self.fc(x))
```

**Output:**
- Node-level risk embeddings (128-dim vectors)
- Risk scores that incorporate dependency context

### 3.3 NLP Models for Text Analysis

#### BERT-based Transformer (DistilBERT or RoBERTa)

**Why this fits:**

- README and documentation analysis for legitimacy signals
- Detect suspicious or malicious descriptions
- Identify social engineering attempts
- Extract semantic features from code comments

**Features used:**
- Package descriptions
- README files
- Code comments (extracted during static analysis)

**Implementation:**
```python
from transformers import DistilBertTokenizer, DistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

def encode_text(text):
    inputs = tokenizer(text, return_tensors='pt', 
                      truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)  # 768-dim embedding
```

**Fine-tuning approach:**
- Pre-train on general npm package descriptions
- Fine-tune on labeled malicious/benign package descriptions
- Use embeddings as features for ensemble model

**Output:**
- 768-dimensional text embeddings
- Classification logits for description-level risk

### 3.4 Anomaly Detection Models

#### Isolation Forest

**Why this fits:**

- Detects novel/zero-day malicious packages
- No labels required (unsupervised)
- Identifies packages with unusual feature combinations
- Complements supervised models

**Features used:**
- All numerical features
- Particularly effective for behavioral and static analysis features

**Configuration:**
```python
iso_forest = IsolationForest(
    n_estimators=200,
    contamination=0.01,  # assume 1% malicious
    max_features=1.0,
    bootstrap=True
)
```

**Output:** Anomaly scores (-1 to 1, where negative = anomalous)

#### Autoencoder (Deep Learning)

**Why this fits:**

- Learns normal package behavior patterns
- Reconstruction error indicates anomalies
- Captures complex non-linear patterns
- Good for high-dimensional feature spaces

**Architecture:**
```python
class PackageAutoencoder(torch.nn.Module):
    def __init__(self, input_dim=50):
        super().__init__()
        self.encoder = torch.nn.Sequential(
            torch.nn.Linear(input_dim, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, 16),
            torch.nn.ReLU(),
            torch.nn.Linear(16, 8)
        )
        self.decoder = torch.nn.Sequential(
            torch.nn.Linear(8, 16),
            torch.nn.ReLU(),
            torch.nn.Linear(16, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, input_dim)
        )
    
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded
```

**Training:** Train only on benign packages

**Output:** Reconstruction error as anomaly score

### 3.5 Ensemble Strategy (Recommended Final Model)

Combine all models using stacking ensemble:

```python
# Level 0 models (base learners)
base_models = {
    'xgboost': xgb_model,
    'random_forest': rf_model,
    'logistic_regression': lr_model,
    'gnn': gnn_model,           # outputs graph embedding + score
    'bert': bert_classifier,     # outputs text embedding + score
    'isolation_forest': iso_forest,
    'autoencoder': autoencoder   # outputs anomaly score
}

# Level 1 model (meta-learner)
meta_model = XGBClassifier(
    objective='binary:logistic',
    max_depth=3,
    learning_rate=0.1,
    n_estimators=100
)

# Combine predictions
def ensemble_predict(package_features):
    predictions = []
    for name, model in base_models.items():
        pred = model.predict_proba(package_features)
        predictions.append(pred)
    
    meta_features = np.concatenate(predictions, axis=1)
    final_risk = meta_model.predict_proba(meta_features)[:, 1]
    return final_risk
```

**Why this works:**
- Combines strengths of different model types
- XGBoost handles tabular features
- GNN captures dependency context
- BERT analyzes textual signals
- Anomaly detection catches novel threats
- Meta-learner learns optimal weighting

---

## 4. Risk Score Output Definition

### 4.1 Risk Score Scale

**Primary Output:** Continuous Risk Score (0-100)

- **0-20:** Low Risk (green)
- **21-50:** Medium Risk (yellow)
- **51-75:** High Risk (orange)
- **76-100:** Critical Risk (red)

**Alternatively:** Probability Score (0-1)
- 0.0 = definitely benign
- 1.0 = definitely malicious
- Can be scaled to 0-100 by multiplying by 100

### 4.2 Risk Classification

Map continuous scores to discrete categories:

```python
def classify_risk(score):
    if score <= 20:
        return "Low"
    elif score <= 50:
        return "Medium"
    elif score <= 75:
        return "High"
    else:
        return "Critical"
```

### 4.3 Detailed Output Format

Complete JSON Response:

```json
{
  "package": "express",
  "version": "4.18.2",
  "scan_timestamp": "2025-11-20T10:00:00Z",
  "risk_assessment": {
    "overall_risk_score": 12,
    "risk_level": "Low",
    "confidence": 0.94
  },
  "risk_factors": {
    "vulnerabilities": {
      "score": 0,
      "details": {
        "total_count": 0,
        "critical_count": 0,
        "high_count": 0,
        "unpatched_count": 0
      }
    },
    "suspicious_code": {
      "score": 5,
      "details": {
        "detected": false,
        "obfuscation_level": "low",
        "dangerous_api_usage": "minimal",
        "network_calls_present": false
      }
    },
    "maintainer_risk": {
      "score": 3,
      "details": {
        "account_age": "10+ years",
        "reputation": "excellent",
        "packages_maintained": 45,
        "activity_level": "active"
      }
    },
    "dependency_risk": {
      "score": 8,
      "details": {
        "total_dependencies": 247,
        "vulnerable_dependencies": 2,
        "outdated_dependencies": 12,
        "max_dependency_depth": 8,
        "high_risk_dependencies": []
      }
    },
    "popularity_trust": {
      "score": 2,
      "details": {
        "weekly_downloads": 25000000,
        "github_stars": 60000,
        "community_health": "excellent",
        "documentation_quality": "high"
      }
    },
    "behavioral_signals": {
      "score": 0,
      "details": {
        "install_scripts": false,
        "network_activity": false,
        "filesystem_modifications": "expected",
        "privilege_escalation": false
      }
    }
  },
  "model_predictions": {
    "xgboost_score": 0.08,
    "random_forest_score": 0.12,
    "gnn_score": 0.15,
    "bert_score": 0.05,
    "anomaly_score": 0.10,
    "ensemble_score": 0.12
  },
  "recommendations": [
    "Package appears safe for use",
    "Consider updating 12 outdated dependencies",
    "Monitor for security advisories"
  ],
  "feature_importance": {
    "top_risk_contributors": [
      {"feature": "dependency_depth", "importance": 0.35},
      {"feature": "outdated_dependencies", "importance": 0.28},
      {"feature": "days_since_last_update", "importance": 0.15}
    ]
  }
}
```

### 4.4 Batch Output Format

For scanning multiple packages:

```json
{
  "scan_id": "scan_20251120_001",
  "timestamp": "2025-11-20T10:00:00Z",
  "total_packages": 150,
  "summary": {
    "low_risk": 120,
    "medium_risk": 25,
    "high_risk": 4,
    "critical_risk": 1
  },
  "packages": [
    {
      "package": "express",
      "version": "4.18.2",
      "risk_score": 12,
      "risk_level": "Low"
    },
    {
      "package": "malicious-pkg",
      "version": "1.0.0",
      "risk_score": 95,
      "risk_level": "Critical"
    }
  ],
  "high_priority_alerts": [
    {
      "package": "malicious-pkg",
      "version": "1.0.0",
      "risk_score": 95,
      "reason": "Install script contains network calls to suspicious domain"
    }
  ]
}
```

### 4.5 API Response Format

Real-time API endpoint response:

```json
{
  "status": "success",
  "data": {
    "package": "lodash",
    "version": "4.17.21",
    "risk_score": 8,
    "risk_level": "Low",
    "cached": false,
    "processing_time_ms": 245
  },
  "metadata": {
    "model_version": "v2.3.1",
    "last_updated": "2025-11-15T00:00:00Z",
    "data_sources": ["npm", "nvd", "osv", "github"]
  }
}
```

---

## 5. Deliverables

### 5.1 Detailed Data Collection Plan

**Phase 1: Data Source Integration (Weeks 1-2)**

- Set up npm registry API integration
- Configure vulnerability database access (NVD, OSV, GitHub)
- Implement GitHub API for repository metrics
- Build web scraper for additional sources

**Phase 2: Static & Behavioral Analysis Pipeline (Weeks 3-4)**

Develop static code analysis tools:
- AST parsing for JavaScript/TypeScript
- Pattern matching for dangerous APIs
- Obfuscation detection algorithms

Implement sandboxed execution environment for behavioral analysis:
- Docker containers with network monitoring
- Filesystem change tracking
- Process monitoring

**Phase 3: Graph Construction (Week 5)**
- Build dependency graph extraction pipeline
- Calculate graph metrics (centrality, clustering)
- Store graph in Neo4j or NetworkX format

**Phase 4: Labeled Dataset Creation (Week 6)**

Aggregate known malicious packages from:
- npm security advisories archive
- MalOSS dataset
- Security researcher databases

- Label 10,000+ benign packages (popular, verified)
- Create train/validation/test splits (70/15/15)

**Data Collection Schedule:**

- **Real-time:** npm registry updates (new packages/versions)
- **Daily:** Vulnerability database updates
- **Weekly:** GitHub metrics refresh
- **Monthly:** Full dependency graph recomputation

### 5.2 Data Model / Feature Schema

**Database Schema (MongoDB):**

```javascript
// packages collection
{
  _id: "express@4.18.2",
  name: "express",
  version: "4.18.2",
  collected_at: ISODate("2025-11-20T10:00:00Z"),
  
  metadata: {
    published_at: ISODate(),
    maintainers: [{...}],
    repository: String,
    license: String,
    downloads: {...},
    // ... all metadata fields
  },
  dependencies: {
direct: [{name: String, version: String, risk_score: Number}],
transitive: [{...}],
metrics: {
total_count: Number,
max_depth: Number,
outdated_count: Number,
// ... dependency metrics
},
graph_features: {
centrality: Number,
clustering: Number
}
},
vulnerabilities: [{
cve_id: String,
cvss_score: Number,
severity: String,
published: ISODate(),
fixed_in: String,
exploited: Boolean
}],
static_analysis: {
dangerous_apis: {...},
obfuscation: {...},
code_metrics: {...},
typosquatting: {...}
},
behavioral: {
install_script: {...},
sandbox_execution: {...}
},
popularity: {
github: {...},
npm: {...},
maintainer_activity: {...},
quality_indicators: {...}
},
label: {
classification: String,  // "benign" | "malicious" | "suspicious"
confidence: Number,
source: String,
attack_type: String
},
computed_features: {
feature_vector: [Number],  // ML-ready features
text_embedding: [Number],
graph_embedding: [Number]
}
}
// vulnerability_cache collection (for faster lookups)
{
_id: "CVE-2021-44906",
packages_affected: ["minimist@1.2.5"],
cvss_score: 5.6,
severity: "MODERATE",
// ... vulnerability details
}
// dependency_graph collection
{
_id: ObjectId(),
root_package: "my-app@1.0.0",
nodes: [{id: String, features: [Number]}],
edges: [{source: String, target: String, type: String}],
    computed_at: ISODate()
}
```

**Feature Vector Schema (for ML pipeline):**
```python
feature_schema = {
    # 60 numerical features
    'numerical': {
        'metadata': [
            'account_age_days',
            'packages_maintained',
            'downloads_last_week',
            'downloads_last_month',
            'version_count',
            'days_since_last_update',
            'package_size_bytes',
            'file_count'
        ],
        'dependencies': [
            'total_dependencies',
            'direct_dependencies',
            'max_depth',
            'outdated_count',
            'deprecated_count',
            'avg_dependency_age_days',
            'betweenness_centrality',
            'clustering_coefficient',
            'avg_dependency_risk_score'
        ],
        'vulnerabilities': [
            'total_vulnerabilities',
            'critical_count',
            'high_count',
            'medium_count',
            'low_count',
            'avg_time_to_patch_days',
            'max_cvss_score',
            'avg_cvss_score',
            'unpatched_count'
        ],
        'static_analysis': [
            'eval_usage',
            'child_process_exec',
            'fs_write_count',
            'fs_read_count',
            'network_requests',
            'high_entropy_strings',
            'base64_encoded_count',
            'obfuscation_score',
            'total_loc',
            'cyclomatic_complexity',
            'dead_code_percentage',
            'edit_distance_to_popular',
            'crypto_operations'
        ],
        'behavioral': [
            'install_script_size_bytes',
            'network_calls_count',
            'spawned_processes_count',
            'file_modifications_count',
            'env_access_count'
        ],
        'popularity': [
            'github_stars',
            'github_forks',
            'github_watchers',
            'dependents_count',
            'active_contributors',
            'commits_last_90_days',
            'avg_issue_response_hours',
            'pr_merge_rate',
            'test_coverage',
            'documentation_score'
        ]
    },
    
    # 15 categorical features (will be encoded)
    'categorical': [
        'license',
        'email_domain',
        'repository_host',
        'has_install_scripts',
        'circular_dependencies',
        'is_typosquat_candidate',
        'requires_sudo',
        'network_activity_detected',
        'attempted_privilege_escalation',
        'has_tests',
        'has_ci',
        'has_documentation',
        'has_repository',
        'exploited_vulnerabilities_present',
        'maintainer_account_type'
    ],
    
    # Text features (embeddings)
    'text': {
        'package_description': 'string',
        'readme_content': 'string',
        'code_comments_sample': 'string'
    },
    
    # Graph features
    'graph': {
        'adjacency_list': 'dict',
        'node_features': 'array'
    },
    
    # Target variable
    'target': {
        'is_malicious': 'binary',  # 0 or 1
        'risk_score': 'float'       # 0-100
    }
}
```

### 5.3 Recommended ML Models (Implementation Details)

**Model Architecture Overview:**
Input Data Pipeline:
├── Tabular Features → XGBoost, Random Forest, Logistic Regression
├── Dependency Graph → Graph Neural Network (GAT)
├── Text Features → BERT Transformer
├── Anomaly Detection → Isolation Forest, Autoencoder
└── All predictions → Ensemble Meta-Learner → Final Risk Score

**Model Selection Rationale:**

| Model | Strengths | Weaknesses | Use Case |
|-------|-----------|------------|----------|
| **XGBoost** | Best for tabular data, handles missing values, feature importance | Needs feature engineering | Primary classifier |
| **Random Forest** | Robust, handles outliers, less prone to overfitting | Can be slow on large datasets | Ensemble diversity |
| **GNN (GAT)** | Captures dependency relationships, propagates risk | Requires graph construction | Dependency analysis |
| **BERT** | Understands semantic meaning, detects social engineering | Computationally expensive | Text analysis |
| **Isolation Forest** | Detects novel attacks, unsupervised | May have false positives | Zero-day detection |
| **Autoencoder** | Learns normal patterns, good for high-dim data | Requires tuning threshold | Anomaly detection |

**Training Configuration:**
```python
# Model hyperparameters (optimized via grid search)
model_configs = {
    'xgboost': {
        'n_estimators': 500,
        'max_depth': 10,
        'learning_rate': 0.05,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'scale_pos_weight': 10,  # for imbalanced classes
        'eval_metric': 'auc'
    },
    
    'random_forest': {
        'n_estimators': 300,
        'max_depth': 15,
        'min_samples_split': 10,
        'min_samples_leaf': 5,
        'class_weight': 'balanced',
        'max_features': 'sqrt'
    },
    
    'gnn': {
        'hidden_channels': 128,
        'num_layers': 3,
        'heads': 8,
        'dropout': 0.3,
        'learning_rate': 0.001,
        'epochs': 100
    },
    
    'bert': {
        'model_name': 'distilbert-base-uncased',
        'max_length': 512,
        'batch_size': 16,
        'learning_rate': 2e-5,
        'epochs': 3,
        'warmup_steps': 500
    },
    
    'isolation_forest': {
        'n_estimators': 200,
        'contamination': 0.01,
        'max_samples': 'auto',
        'max_features': 1.0
    },
    
    'autoencoder': {
        'encoding_dim': 8,
        'hidden_layers': [32, 16],
        'activation': 'relu',
        'optimizer': 'adam',
        'epochs': 50,
        'batch_size': 32
    }
}
```

**Feature Importance Analysis:**
```python
# After training XGBoost
import shap

explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

# Top 10 most important features
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False).head(10)

# Expected top features:
# 1. max_cvss_score (vulnerabilities)
# 2. obfuscation_score
# 3. install_script_present
# 4. network_calls_count
# 5. edit_distance_to_popular (typosquatting)
# 6. account_age_days
# 7. downloads_last_week
# 8. total_dependencies
# 9. github_stars
# 10. has_install_scripts
```

### 5.4 Training Pipeline Design

**Complete Training Pipeline Architecture:**
┌─────────────────────────────────────────────────────────────┐
│                    DATA COLLECTION LAYER                     │
├─────────────────────────────────────────────────────────────┤
│  npm API → MongoDB   │  NVD → Postgres  │  GitHub → Cache   │
└─────────────────────────────────────────────────────────────┘
↓
┌─────────────────────────────────────────────────────────────┐
│                 FEATURE ENGINEERING LAYER                    │
├─────────────────────────────────────────────────────────────┤
│  • Extract numerical features                                │
│  • Encode categorical variables                              │
│  • Generate text embeddings (BERT)                           │
│  • Construct dependency graphs                               │
│  • Normalize and scale features                              │
│  • Handle missing values                                     │
└─────────────────────────────────────────────────────────────┘
↓
┌─────────────────────────────────────────────────────────────┐
│                   MODEL TRAINING LAYER                       │
├─────────────────────────────────────────────────────────────┤
│  Level 0 Models (Train in Parallel):                        │
│  ├── XGBoost (tabular features)                             │
│  ├── Random Forest (tabular features)                       │
│  ├── Logistic Regression (tabular features)                 │
│  ├── GNN (graph features)                                   │
│  ├── BERT (text features)                                   │
│  ├── Isolation Forest (anomaly detection)                   │
│  └── Autoencoder (anomaly detection)                        │
│                                                              │
│  Level 1 Model (Meta-learner):                              │
│  └── XGBoost Ensemble (combines all predictions)            │
└─────────────────────────────────────────────────────────────┘
↓
┌─────────────────────────────────────────────────────────────┐
│                   EVALUATION & DEPLOYMENT                    │
├─────────────────────────────────────────────────────────────┤
│  • Cross-validation (5-fold)                                │
│  • Test set evaluation                                       │
│  • Model serialization (pickle/ONNX)                        │
│  • Deploy to production API                                  │
└─────────────────────────────────────────────────────────────┘
